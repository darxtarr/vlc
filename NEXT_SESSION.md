# Next Session: Complete M2 GPU Integration

**Created**: 2025-10-01
**For**: Next Sonnet session
**Goal**: Complete M2 GPU integration (estimated 1-2 hours)
**Current**: 95% complete architecture, API validation needed

---

## üéØ Mission: Get GPU Compression Working

You're inheriting **excellent work at 95% completion**. The CPU implementation works beautifully (2-3% compression), GPU architecture is professional, and `compress_gpu()` is already integrated. Just need to validate WGPU API calls and complete the pipeline.

---

## ‚ö° Quick Start Checklist

### Step 1: Validate WGPU API (30 min)
- [ ] Fix device polling in `src/gpu/context.rs:202` and `src/gpu/ops.rs:223`
- [ ] Fix adapter error handling in `src/gpu/context.rs:28-34`
- [ ] Validate memory_hints field in `src/gpu/context.rs:42`
- [ ] Fix NonZero type in `src/gpu/ops.rs:183`
- [ ] Run `cargo check` - should compile

### Step 2: Test GPU Initialization (15 min)
- [ ] Add simple test in `src/gpu/ops.rs`:
```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_gpu_init() {
        let ctx = GpuContext::new().await.unwrap();
        assert!(ctx.device.limits().max_compute_workgroup_size_x >= 256);
    }
}
```
- [ ] Run `cargo test` - should pass

### Step 3: Complete GPU Operations (45 min)
- [ ] Implement `reduce_stats()` in `src/gpu/ops.rs` (copy pattern from assign_points)
- [ ] Implement `update_anchors()` in `src/gpu/ops.rs` (copy pattern from assign_points)
- [ ] Wire both into `compress_gpu()` loop in `src/anneal.rs`

### Step 4: Validate & Celebrate (30 min)
- [ ] Add GPU test command to CLI: `cargo run --bin vlc test-gpu`
- [ ] Compare CPU vs GPU results (should match exactly)
- [ ] Run benchmark, measure speedup
- [ ] Update STATUS.md: M2 ‚úÖ COMPLETE
- [ ] Commit and celebrate! üéâ

---

## üîß Specific WGPU API Fixes Needed

### Fix 1: Device Polling (2 locations)
**File**: `src/gpu/context.rs:202` and `src/gpu/ops.rs:223`

**Current**:
```rust
let _ = self.context.device.poll(wgpu::Maintain::Wait);
```

**Check**: Validate enum variant exists. If not, try:
```rust
self.context.device.poll(wgpu::MaintainBase::Wait);
// or
self.context.device.poll(wgpu::MaintainResult::Wait);
```

**Quick test**: Open `src/gpu/ops.rs` and see what the compiler suggests.

---

### Fix 2: Adapter Request Error Handling
**File**: `src/gpu/context.rs:28-34`

**Current**:
```rust
let adapter = instance
    .request_adapter(&RequestAdapterOptions { ... })
    .await?;
```

**Problem**: `request_adapter()` returns `Option<Adapter>`, not `Result`

**Fix**:
```rust
let adapter = instance
    .request_adapter(&RequestAdapterOptions {
        power_preference: PowerPreference::HighPerformance,
        compatible_surface: None,
        force_fallback_adapter: false,
    })
    .await
    .ok_or("No suitable GPU adapter found")?;
```

---

### Fix 3: Memory Hints
**File**: `src/gpu/context.rs:42`

**Current**:
```rust
memory_hints: Default::default(),
```

**Check**: If field doesn't exist in DeviceDescriptor, remove it entirely:
```rust
let (device, queue) = adapter
    .request_device(
        &DeviceDescriptor {
            label: Some("VLC GPU Device"),
            required_features: Features::empty(),
            required_limits: Limits::default(),
            // memory_hints: Default::default(),  // Remove if doesn't exist
            trace: wgpu::Trace::Off,
        }
    )
    .await?;
```

---

### Fix 4: NonZero Buffer Size
**File**: `src/gpu/ops.rs:183`

**Current**:
```rust
size: std::num::NonZero::new(std::mem::size_of::<AssignParams>() as u64),
```

**Problem**: Might need specific NonZeroU64 type

**Fix**:
```rust
size: std::num::NonZeroU64::new(std::mem::size_of::<AssignParams>() as u64),
```

---

## üìù Implement Remaining GPU Operations

### reduce_stats() Implementation

Add to `src/gpu/ops.rs` (copy pattern from assign_points):

```rust
pub async fn reduce_stats(
    &mut self,
    points: &[f16],
    assignments: &Assignments,
    anchors: &AnchorSet,
    n: usize,
    m: usize,
    d: usize,
) -> Result<Vec<AnchorStats>, Box<dyn std::error::Error>> {
    self.ensure_buffers(n, m, d);

    // Convert f16 to f32
    let points_f32: Vec<f32> = points.iter().map(|&x| x.to_f32()).collect();

    // Upload buffers
    self.context.queue.write_buffer(
        self.points_buffer.as_ref().unwrap(),
        0,
        bytemuck::cast_slice(&points_f32),
    );

    self.context.queue.write_buffer(
        self.assigns_buffer.as_ref().unwrap(),
        0,
        bytemuck::cast_slice(&assignments.assign),
    );

    // Upload params (n, m, d, huber_threshold)
    let params = ReduceParams {
        n: n as u32,
        m: m as u32,
        d: d as u32,
        huber_threshold: 1.0,
    };
    self.context.queue.write_buffer(
        &self.params_buffer,
        0,
        bytemuck::cast_slice(&[params]),
    );

    // Create bind groups (copy from assign_points pattern)
    // ...

    // Dispatch: m workgroups (one per anchor)
    let mut encoder = self.context.device.create_command_encoder(&Default::default());
    {
        let mut compute_pass = encoder.begin_compute_pass(&Default::default());
        compute_pass.set_pipeline(&self.context.reduce_pipeline);
        compute_pass.set_bind_group(0, &storage_bind_group, &[]);
        compute_pass.set_bind_group(1, &uniform_bind_group, &[]);

        compute_pass.dispatch_workgroups(m as u32, 1, 1);  // One workgroup per anchor
    }

    self.context.queue.submit([encoder.finish()]);
    let _ = self.context.device.poll(wgpu::Maintain::Wait);

    // Read back stats buffer [m √ó (d+2)]
    let staging_buffer = self.context.create_staging_buffer(
        (m * (d + 2) * std::mem::size_of::<f32>()) as u64
    );

    // Copy and map (copy from assign_points pattern)
    // ...

    // Convert to AnchorStats
    let mut result = Vec::with_capacity(m);
    for anchor_idx in 0..m {
        let base = anchor_idx * (d + 2);
        let mean = stats_data[base..base+d].to_vec();
        let count = stats_data[base+d] as usize;
        let variance = vec![stats_data[base+d+1]; d]; // Simplified

        result.push(AnchorStats { mean, count, variance });
    }

    Ok(result)
}
```

---

### update_anchors() Implementation

Add to `src/gpu/ops.rs`:

```rust
pub async fn update_anchors(
    &mut self,
    anchors: &mut AnchorSet,
    stats: &[AnchorStats],
    temperature: f32,
    learning_rate: f32,
) -> Result<(), Box<dyn std::error::Error>> {
    let m = anchors.m;
    let d = anchors.d;

    // Convert anchors to f32
    let mut anchors_f32: Vec<f32> = anchors.anchors.iter().map(|&x| x.to_f32()).collect();

    // Convert stats to flat buffer [m √ó (d+2)]
    let mut stats_f32 = Vec::with_capacity(m * (d + 2));
    for stat in stats {
        stats_f32.extend(&stat.mean);
        stats_f32.push(stat.count as f32);
        stats_f32.push(stat.variance.iter().sum::<f32>() / d as f32); // avg variance
    }

    // Upload buffers
    self.context.queue.write_buffer(
        self.anchors_buffer.as_ref().unwrap(),
        0,
        bytemuck::cast_slice(&anchors_f32),
    );

    self.context.queue.write_buffer(
        self.stats_buffer.as_ref().unwrap(),
        0,
        bytemuck::cast_slice(&stats_f32),
    );

    // Upload params
    let params = UpdateParams {
        m: m as u32,
        d: d as u32,
        temperature,
        learning_rate,
        momentum: 0.0,
        enable_jacobians: 0,
        _padding: [0, 0],
    };
    self.context.queue.write_buffer(
        &self.params_buffer,
        0,
        bytemuck::cast_slice(&[params]),
    );

    // Create bind groups and dispatch
    // Dispatch: (m * d) threads total
    let total_threads = (m * d) as u32;
    let workgroups = (total_threads + 255) / 256;

    let mut encoder = self.context.device.create_command_encoder(&Default::default());
    {
        let mut compute_pass = encoder.begin_compute_pass(&Default::default());
        compute_pass.set_pipeline(&self.context.update_pipeline);
        compute_pass.set_bind_group(0, &storage_bind_group, &[]);
        compute_pass.set_bind_group(1, &uniform_bind_group, &[]);

        compute_pass.dispatch_workgroups(workgroups, 1, 1);
    }

    self.context.queue.submit([encoder.finish()]);
    let _ = self.context.device.poll(wgpu::Maintain::Wait);

    // Read back updated anchors (if needed for CPU sync)
    // For now, copy the pattern from assign_points

    Ok(())
}
```

---

### Wire into compress_gpu()

**File**: `src/anneal.rs` in the `compress_gpu()` function

**Current loop** (only has assign_points):
```rust
while state.iteration < config.max_iterations && !state.converged {
    let old_assignments = assignments.clone();

    // Step 1: Assign (GPU) ‚úÖ
    assignments = gpu_ops.assign_points(points, &anchors, n, d).await?;

    // Step 2: Reduce (CPU - needs GPU)
    let stats = ops::compute_robust_stats(...);

    // Step 3: Update (CPU - needs GPU)
    ops::update_anchors(...);

    // ...
}
```

**Updated loop** (full GPU):
```rust
while state.iteration < config.max_iterations && !state.converged {
    let old_assignments = assignments.clone();

    // Step 1: Assign points (GPU)
    assignments = gpu_ops.assign_points(points, &anchors, n, d).await?;

    // Step 2: Compute statistics (GPU)
    let stats = gpu_ops.reduce_stats(points, &assignments, &anchors, n, anchors.m, d).await?;

    // Step 3: Update anchors (GPU)
    gpu_ops.update_anchors(&mut anchors, &stats, state.temperature, config.learning_rate).await?;

    // Rest stays the same (energy, convergence, etc.)
    state.energy = ops::compute_energy(points, &anchors, &assignments);
    state.assignment_changes = ops::count_assignment_changes(&old_assignments, &assignments);
    // ...
}
```

---

## üß™ Testing Strategy

### Test 1: GPU Initialization
```bash
cargo test --lib gpu::
```

Should see GpuContext initialize without errors.

### Test 2: CPU vs GPU Correctness
Add to `src/anneal.rs`:

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_gpu_matches_cpu() {
        let n = 100;
        let d = 32;
        let m = 5;

        // Generate test data
        let points: Vec<f16> = (0..n*d)
            .map(|i| f16::from_f32((i as f32).sin()))
            .collect();

        let config = AnnealingConfig {
            m,
            max_iterations: 10,
            ..Default::default()
        };

        // Run both
        let cpu_result = compress(&points, n, d, config.clone());
        let gpu_result = compress_gpu(&points, n, d, config).await.unwrap();

        // Compare assignments
        assert_eq!(cpu_result.assignments.assign.len(),
                   gpu_result.assignments.assign.len());

        // Energy should be similar (within 1%)
        let energy_diff = (cpu_result.metadata.final_energy -
                          gpu_result.metadata.final_energy).abs();
        assert!(energy_diff < 0.01);
    }
}
```

### Test 3: CLI GPU Test
Add to `src/bin/vlc.rs`:

```rust
"test-gpu" => {
    println!("Running GPU compression test...");

    // Generate test data
    let (n, d, m) = (1000, 64, 10);
    let points = generate_synthetic_data(n, d);

    let config = vlc::AnnealingConfig {
        m,
        max_iterations: 50,
        ..Default::default()
    };

    // Time GPU
    let start = std::time::Instant::now();
    let gpu_result = tokio::runtime::Runtime::new()
        .unwrap()
        .block_on(vlc::compress_gpu(&points, n, d, config.clone()))
        .unwrap();
    let gpu_time = start.elapsed();

    // Time CPU
    let start = std::time::Instant::now();
    let cpu_result = vlc::compress(&points, n, d, config);
    let cpu_time = start.elapsed();

    println!("GPU time: {:?}", gpu_time);
    println!("CPU time: {:?}", cpu_time);
    println!("Speedup: {:.2}x", cpu_time.as_secs_f64() / gpu_time.as_secs_f64());
}
```

Run:
```bash
cargo run --bin vlc test-gpu
```

---

## üìä Success Criteria

Before declaring M2 complete:

- [ ] `cargo check` passes without errors
- [ ] `cargo test` passes all tests
- [ ] GPU initialization works (no device errors)
- [ ] GPU results match CPU results (assignments identical)
- [ ] Measurable speedup on 10K points (aim for 2-5x minimum)
- [ ] No memory leaks (run with valgrind if needed)
- [ ] STATUS.md updated: M2 ‚úÖ COMPLETE

---

## üö® If You Get Stuck

### Problem: Can't find correct WGPU enum variant
**Solution**:
```bash
# Check WGPU docs
cargo doc --open
# Navigate to wgpu crate, search for the type
```

### Problem: Buffer mapping fails
**Solution**: Ensure buffer has `MAP_READ` usage:
```rust
BufferUsages::MAP_READ | BufferUsages::COPY_DST
```

### Problem: Shader compilation error
**Solution**: Check shader with:
```bash
# Save shader to file
cargo run --bin vlc test-gpu 2>&1 | grep -A 10 "shader"
```

### Problem: Results don't match CPU
**Solution**:
1. Test with tiny data (10 points, 2 anchors)
2. Print intermediate values
3. Check f16‚Üîf32 conversions
4. Verify buffer layouts match shader expectations

---

## üìÅ Key Files Reference

**Main files to edit**:
- `src/gpu/context.rs` - Fix API calls (4 fixes)
- `src/gpu/ops.rs` - Add reduce_stats() and update_anchors()
- `src/anneal.rs` - Wire GPU ops into compress_gpu()
- `src/bin/vlc.rs` - Add test-gpu command
- `STATUS.md` - Update when complete

**Don't touch**:
- Shaders (already correct)
- CPU ops (reference implementation)
- Core types (already GPU-aligned)

---

## üéØ Time Estimate

- WGPU API fixes: **30 min**
- Implement reduce_stats: **20 min**
- Implement update_anchors: **25 min**
- Wire into compress_gpu: **10 min**
- Testing & validation: **30 min**
- Documentation update: **5 min**

**Total: ~2 hours** to complete M2 üöÄ

---

## üéâ When Complete

1. Run full test suite:
```bash
cargo test
cargo run --bin vlc test
cargo run --bin vlc test-gpu
```

2. Update STATUS.md:
```markdown
## M2: GPU Acceleration ‚úÖ (COMPLETE)
- GPU compression working
- Speedup: Xx over CPU
- All operations GPU-accelerated
```

3. Commit:
```bash
git add -A
git commit -m "Complete M2 GPU integration - full GPU acceleration working"
git push
```

4. Celebrate! You've completed professional GPU compute infrastructure! üéä

---

## üí° Pro Tips

1. **Test incrementally**: Don't implement both ops before testing first one
2. **Use CPU as oracle**: Compare every GPU result to CPU
3. **Print buffer sizes**: Ensure they match expectations
4. **Start small**: 10 points, 2 anchors, then scale up
5. **Watch for alignment**: f32 = 4 bytes, pad structs to 16-byte boundaries

---

## üìö Resources

- **WGPU Docs**: `cargo doc --open` ‚Üí wgpu crate
- **Current Status**: `STATUS.md`
- **Architecture**: `docs/DESIGN.md`
- **Kernel Specs**: `docs/KERNELS.md`
- **This Guide**: `docs/M2_HANDOVER.md`

---

**You've got this!** The hardest work is done. Just wire up the remaining pieces and validate. The GPU is waiting to show you that 10x speedup! üöÄ

*Prepared with care by Claude Code*
*Ready for M2 completion - Let's ship this!* ‚ú®
